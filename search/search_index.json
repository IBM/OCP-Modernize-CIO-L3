{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WATCH THIS DEMONSTRATION Louisa Muschal (Product Manager, IBM Hyper Protect Services) explains the value of IBM Hyper Protect Services for clients and the marketplace. Additional ways to watch: Seismic replay available for download. Chief Innovation Officer \u2014 A Day in the Life Amy , the Chief Innovation Officer (CIO) for Stacked Corp. , has a formidable task before her: overseeing a rapid and seamless modernization effort of the company\u2019s legacy application and services stack, as part of Stacked\u2019s digital transformation strategy. Amy\u2019s team encompasses a range of disciplines, areas of specialization, and scope of responsibilities. It includes: developers and programmers, building out the next generation of applications while also maintaining legacy code; IT administrators, tasked with migrating the business into a new hybrid multicloud environment running on IBM Power Systems; as well as data scientists and other subject matter experts, who require uninterrupted access to the company\u2019s services and data while this modernization effort is underway. This is a company in the early days of their digital modernization efforts. How did they get here and where will they go next? The modernization initiative that the CIO\u2019s office has helmed didn\u2019t emerge from a vacuum. Stacked Corporation\u2019s strategy mirrors the experience of nearly every client (in nearly every industry) that you\u2019ll speak with today: they are either spending money to save money or spending money to make money. Economics are a hugely compelling force behind why every enterprise is looking to digitally modernize today (although certainly not the only reason.) When you spend it to save it, you\u2019re renovating ; and when you spend it to make it, you\u2019re innovating . The best strategies will do both and leverage cost savings from renovating the IT landscape (spending money to save it) to partially fund the innovation (spending money to make it). To illustrate this model, we\u2019ll use something we call a Data Acumen Curve . Note the emergence of a natural border between renovating and innovating . Every client needs to be able to derive downstream benefits of a renovation project, in order to justify the modernization efforts. This simple framework is handy for any strategic investment decisions a company will face across almost any domain. There are several key takeaways from the Data Acumen Curve : Modernization strategy that focus solely on cost reduction deliver shortchanged value. Wait, didn\u2019t you just say that economics are a huge driver behind many enterprise\u2019s push towards digital modernization? Indeed it is. But placing too strong an emphasis on renovation without innovation can be detrimental. Today\u2019s technology is about efficiency, automation, optimization, and more. If you\u2019re focused only on cutting costs, the value returned won\u2019t be enormous. Don\u2019t get us wrong, smart cost cutting is a terrific strategy and you can forward the money saved toward more renovation and compound that value into innovation, but that is not the end game (despite so many being forced to play it as if it was). Governance is a great example. Most organizations scurry to implement regulatory compliance with the least possible work to comply approach\u2014mainly, avoidance of fines (cost savings). However, this approach shortchanges the value of such a project because it misses the opportunity to create regulatory dividends from those compliance investments (data lineage, for example) to accelerate analytics strategy. Most haven\u2019t bent the curve for maximum value. When you stop and take stock of where your organization truly is on an Acumen Curve, most will wake up and think \u201cWow, we have a long way to go!\u201d We doubt few of the businesses that turned into divers, or had to contend with new arrivers, had any idea how truly broken their digital strategy was until COVID and disruption hit. That\u2019s the point of these curves; you need to know ahead of time, not after the fact. Real value comes in the innovation phase. Client's cannot modernize if all they're doing is spending money to save or make it, without also reimagining the way they work. Most organizations today are unwilling to do the deep rethinking of their business models and workflows that will allow them to fully embrace the opportunities presented to them by an innovation investment. As clients like Stacked develop their Acumen Curves, they must reimagine business processes from the ground up with new capabilities \u2014 we guarantee you that the impact will be greater for having done so. We created the Cloud Acumen Curve (Figure 1 above) for plotting value as a business moves from monoliths to microservices (i.e. undergoing digital transformation and modernization). Note that the four quadrants are different from the curve shown earlier in Figure 2. Legacy (monolithic) apps refer to those applications where the entire codebase is combined into a single program that runs in one place. Uber\u2019s app, on the other hand, would sit somewhere on the far-right quadrant of this curve: it is made up of perhaps dozens, if not hundreds, of microservices that have very discrete jobs to perform. It\u2019s important to remember that some apps should stay the way they are because you have \u201cbigger fish to fry\u201d; your business may be looking for quick wins rather than long-haul (albeit worthwhile) projects, while some apps are too critical to the business to entertain the expense or risk of changing just yet. These apps will retain ongoing support as legacy code. This naturally brings us to the middle phases on your journey to derive the most value you can from the cloud. This is where functions like archive, Q/A, and test have been replatformed to the cloud, resulting in real value drivers such as CI/CD. Mission-critical apps can replatform to the cloud, too. For example, we\u2019ve seen businesses with critical functions (running on-premises on AIX and IBM i servers) become modernized and reap public cloud benefits such as pay-as-you-go billing, self-service provisioning, and flexible management, without having to change the code. This approach helps you grow at your own pace and start a cloud journey without heavy up-front costs, allows your workloads to run when and where you want, and more. The takeaway? As you move farther and farther to the right of the Cloud Acumen Curve (allowing digital properties to sit where they make sense) you gain more and more cloud acumen, which delivers more and more value to your business. There\u2019s a final consideration we want to explicitly note here: how partnerships matter. As you build out your cloud strategy, it is critical that it be open and portable: being able to build on one cloud property and move it to another either because of specific compute needs (resiliency, GPUs, quantum, and so on), vendor disagreement, or just having that option to negotiate better terms and pricing\u2014flexibility is key. Quite simply, the assets you create should be able to be deployed anywhere across your landscape to fully realize the benefits of your hard work. It's essential for every business to cultivate a cloud and digital modernization acumen so that your business leaders know what to look for, understand how to build teams, and recognize the benefits of your transformation efforts. If we\u2019ve been successful in persuading you, we think you\u2019ll end up developing a modified Cloud Acumen Curve for your business similar to what you see in Figure 3, shown above. Compare the curve in Figure 3 to the two previous curves shown (Figure 1 and 2). It bends drastically in the renovation phase because your business has enacted a digital transformation and modernization plan in support of it. Your organization creates downstream dividends from that plan and the work done in renovation drives innovation . We can\u2019t stress enough how useful this model has been with the clients to whom we\u2019ve been successful in delivering value. These acumen curves are how we get projects and conversations going, and how we hold each other accountable.","title":"Introduction"},{"location":"#_1","text":"","title":""},{"location":"#chief-innovation-officer-a-day-in-the-life","text":"Amy , the Chief Innovation Officer (CIO) for Stacked Corp. , has a formidable task before her: overseeing a rapid and seamless modernization effort of the company\u2019s legacy application and services stack, as part of Stacked\u2019s digital transformation strategy. Amy\u2019s team encompasses a range of disciplines, areas of specialization, and scope of responsibilities. It includes: developers and programmers, building out the next generation of applications while also maintaining legacy code; IT administrators, tasked with migrating the business into a new hybrid multicloud environment running on IBM Power Systems; as well as data scientists and other subject matter experts, who require uninterrupted access to the company\u2019s services and data while this modernization effort is underway. This is a company in the early days of their digital modernization efforts. How did they get here and where will they go next? The modernization initiative that the CIO\u2019s office has helmed didn\u2019t emerge from a vacuum. Stacked Corporation\u2019s strategy mirrors the experience of nearly every client (in nearly every industry) that you\u2019ll speak with today: they are either spending money to save money or spending money to make money. Economics are a hugely compelling force behind why every enterprise is looking to digitally modernize today (although certainly not the only reason.) When you spend it to save it, you\u2019re renovating ; and when you spend it to make it, you\u2019re innovating . The best strategies will do both and leverage cost savings from renovating the IT landscape (spending money to save it) to partially fund the innovation (spending money to make it). To illustrate this model, we\u2019ll use something we call a Data Acumen Curve . Note the emergence of a natural border between renovating and innovating . Every client needs to be able to derive downstream benefits of a renovation project, in order to justify the modernization efforts. This simple framework is handy for any strategic investment decisions a company will face across almost any domain. There are several key takeaways from the Data Acumen Curve : Modernization strategy that focus solely on cost reduction deliver shortchanged value. Wait, didn\u2019t you just say that economics are a huge driver behind many enterprise\u2019s push towards digital modernization? Indeed it is. But placing too strong an emphasis on renovation without innovation can be detrimental. Today\u2019s technology is about efficiency, automation, optimization, and more. If you\u2019re focused only on cutting costs, the value returned won\u2019t be enormous. Don\u2019t get us wrong, smart cost cutting is a terrific strategy and you can forward the money saved toward more renovation and compound that value into innovation, but that is not the end game (despite so many being forced to play it as if it was). Governance is a great example. Most organizations scurry to implement regulatory compliance with the least possible work to comply approach\u2014mainly, avoidance of fines (cost savings). However, this approach shortchanges the value of such a project because it misses the opportunity to create regulatory dividends from those compliance investments (data lineage, for example) to accelerate analytics strategy. Most haven\u2019t bent the curve for maximum value. When you stop and take stock of where your organization truly is on an Acumen Curve, most will wake up and think \u201cWow, we have a long way to go!\u201d We doubt few of the businesses that turned into divers, or had to contend with new arrivers, had any idea how truly broken their digital strategy was until COVID and disruption hit. That\u2019s the point of these curves; you need to know ahead of time, not after the fact. Real value comes in the innovation phase. Client's cannot modernize if all they're doing is spending money to save or make it, without also reimagining the way they work. Most organizations today are unwilling to do the deep rethinking of their business models and workflows that will allow them to fully embrace the opportunities presented to them by an innovation investment. As clients like Stacked develop their Acumen Curves, they must reimagine business processes from the ground up with new capabilities \u2014 we guarantee you that the impact will be greater for having done so. We created the Cloud Acumen Curve (Figure 1 above) for plotting value as a business moves from monoliths to microservices (i.e. undergoing digital transformation and modernization). Note that the four quadrants are different from the curve shown earlier in Figure 2. Legacy (monolithic) apps refer to those applications where the entire codebase is combined into a single program that runs in one place. Uber\u2019s app, on the other hand, would sit somewhere on the far-right quadrant of this curve: it is made up of perhaps dozens, if not hundreds, of microservices that have very discrete jobs to perform. It\u2019s important to remember that some apps should stay the way they are because you have \u201cbigger fish to fry\u201d; your business may be looking for quick wins rather than long-haul (albeit worthwhile) projects, while some apps are too critical to the business to entertain the expense or risk of changing just yet. These apps will retain ongoing support as legacy code. This naturally brings us to the middle phases on your journey to derive the most value you can from the cloud. This is where functions like archive, Q/A, and test have been replatformed to the cloud, resulting in real value drivers such as CI/CD. Mission-critical apps can replatform to the cloud, too. For example, we\u2019ve seen businesses with critical functions (running on-premises on AIX and IBM i servers) become modernized and reap public cloud benefits such as pay-as-you-go billing, self-service provisioning, and flexible management, without having to change the code. This approach helps you grow at your own pace and start a cloud journey without heavy up-front costs, allows your workloads to run when and where you want, and more. The takeaway? As you move farther and farther to the right of the Cloud Acumen Curve (allowing digital properties to sit where they make sense) you gain more and more cloud acumen, which delivers more and more value to your business. There\u2019s a final consideration we want to explicitly note here: how partnerships matter. As you build out your cloud strategy, it is critical that it be open and portable: being able to build on one cloud property and move it to another either because of specific compute needs (resiliency, GPUs, quantum, and so on), vendor disagreement, or just having that option to negotiate better terms and pricing\u2014flexibility is key. Quite simply, the assets you create should be able to be deployed anywhere across your landscape to fully realize the benefits of your hard work. It's essential for every business to cultivate a cloud and digital modernization acumen so that your business leaders know what to look for, understand how to build teams, and recognize the benefits of your transformation efforts. If we\u2019ve been successful in persuading you, we think you\u2019ll end up developing a modified Cloud Acumen Curve for your business similar to what you see in Figure 3, shown above. Compare the curve in Figure 3 to the two previous curves shown (Figure 1 and 2). It bends drastically in the renovation phase because your business has enacted a digital transformation and modernization plan in support of it. Your organization creates downstream dividends from that plan and the work done in renovation drives innovation . We can\u2019t stress enough how useful this model has been with the clients to whom we\u2019ve been successful in delivering value. These acumen curves are how we get projects and conversations going, and how we hold each other accountable.","title":"Chief Innovation Officer \u2014 A Day in the Life"},{"location":"evaluation/","text":"Evaluation Criteria for IBM Technical Sellers and Business Partners The series of hands-on tutorials and learning modules embedded in this Level 3 course are designed to provide IBM Technical Sellers and Business Partners with the fluency to gain trusted advisor status with clients and the expertise to tailor live technological demonstrations for customers. The hands-on lab will examine a day in the life of Amy, a Chief Innovation Officer (CIO), and her team as they embark on a digital transformation of Stacked Corporation 's business. Their work and expertise encompass everything from application developers, to IT administrators, to data scientists. As such, DevOps and IT Ops will be a heavy focus of this course\u2014 with security, observability, and automation topics to follow in future hands-on learning. Through the lens of a Chief Innovation Officer, you will: Module A : Enable continuous deployment via Red Hat OpenShift S2I (Source-2-Image) and GitHub webhooks Module B : Graphically (using the OpenShift web dashboard) create, scale, upgrade, and rollback an application with Red Hat OpenShift Module C : Programmatically (using a built-in command line interface) create, scale, upgrade, and rollback an application with Red Hat OpenShift Each module of this course and hands-on lab work should take an estimated 60 minutes to complete (or less depending on your technical expediency). In total, the lab work should take you no more than 5 hours . To receive a Level 3 accreditation, IBMers and Business Partners must demonstrate mastery of the skills learned throughout the various modules of these hands-on labs and coursework. Level 3 accreditation requirements\u2014 and the way participants will be evaluated before receiving accreditation \u2014differs depending on job role. IBM TECHNICAL SELLERS IBM Sales and Tech Sales must develop and record a Stand & Deliver presentation, which will be uploaded to the IBM Stand & Deliver platform for evaluation. This video is intended to simulate your delivery of a \u201clive\u201d demo in front of a client \u2014 on camera. IBMers will have flexibility in defining a hypothetical client, the pain points that customer has, and the goals they aspire to. The recording will then cover the seller\u2019s hands-on demonstration and pitch to the client of the value of the IBM solution using the environments and techniques of this lab. BUSINESS PARTNERS Business Partners must pass an accreditation quiz after completing the hands-on portion of the course. The quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. The quiz questions will ask you about on-screen text or descriptions that come up as you work through the lab guide. Answers to quiz questions can only be determined by carefully following the instructions of the hands-on lab. Sections of the lab marked as OPTIONAL will not be tested on as part of the quiz. You will have three attempts in total to obtain a passing grade. IBMer Stand & Deliver Assessment IBMers \u2014 SUBMIT RECORDINGS HERE Submit your Stand & Deliver recording online using IBM YourLearning: https://yourlearning.ibm.com/activity/ISND-SALESENABLEMENT-111810?planId=PLAN-66BD32D205CA&sectionId=SECTION-A&planIdFromParentTab=PLAN-66BD32D205CA&sectionIdFromParentTab=SECTION-A&planIdForChildTab=PLAN-66BD32D205CA The evaluation criteria described below and on Seismic only applies to IBMers , who must record a Stand & Deliver to receive accreditation for this Level 3 course. By default, IBMers will be evaluated by their First Line Manager (FLM) \u2014 although they may request another manager to evaluate their Stand & Deliver, if appropriate. Instructions on how to submit a Stand & Deliver are included on this page. Sellers must include each of the following in their recording: Seller articulated client's pain point(s) and the value proposition of using OpenShift on IBM Power Systems. Seller highlighted use cases for OpenShift on Power. Seller demonstrated and discussed several of the key differentiated capabilities of OpenShift on Power that deliver on the value proposition on point one. Seller highlighted benefits to the client (this is the why the client can\u2019t live without these benefits section). Seller highlighted benefits to the client's customers (what will the client be able to deliver to their customers that they could not without this product). Seller closed the demo with a call to action for the client that could include: a workshop, a deeper dive into the product meeting, MVP engagements, and so on. Business Partner Quiz Assessment PARTNERS \u2014 COMPLETE ASSESSMENT HERE Complete your assessment online using IBM Training: https://learn.ibm.com/mod/subcourse/view.php?id=217396 The accreditation quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. The quiz questions will ask you about on-screen text or descriptions that come up as you work through the lab guide. Answers to the quiz can only be determined by carefully following the instructions of the hands-on lab. Sections of the lab marked as OPTIONAL will not be tested on as part of the quiz. You will have three attempts in total to obtain a passing grade. Next steps In the following section, you will prepare a hands-on lab environment with the necessary services and configurations.","title":"Evaluation"},{"location":"evaluation/#evaluation-criteria-for-ibm-technical-sellers-and-business-partners","text":"The series of hands-on tutorials and learning modules embedded in this Level 3 course are designed to provide IBM Technical Sellers and Business Partners with the fluency to gain trusted advisor status with clients and the expertise to tailor live technological demonstrations for customers. The hands-on lab will examine a day in the life of Amy, a Chief Innovation Officer (CIO), and her team as they embark on a digital transformation of Stacked Corporation 's business. Their work and expertise encompass everything from application developers, to IT administrators, to data scientists. As such, DevOps and IT Ops will be a heavy focus of this course\u2014 with security, observability, and automation topics to follow in future hands-on learning. Through the lens of a Chief Innovation Officer, you will: Module A : Enable continuous deployment via Red Hat OpenShift S2I (Source-2-Image) and GitHub webhooks Module B : Graphically (using the OpenShift web dashboard) create, scale, upgrade, and rollback an application with Red Hat OpenShift Module C : Programmatically (using a built-in command line interface) create, scale, upgrade, and rollback an application with Red Hat OpenShift Each module of this course and hands-on lab work should take an estimated 60 minutes to complete (or less depending on your technical expediency). In total, the lab work should take you no more than 5 hours . To receive a Level 3 accreditation, IBMers and Business Partners must demonstrate mastery of the skills learned throughout the various modules of these hands-on labs and coursework. Level 3 accreditation requirements\u2014 and the way participants will be evaluated before receiving accreditation \u2014differs depending on job role. IBM TECHNICAL SELLERS IBM Sales and Tech Sales must develop and record a Stand & Deliver presentation, which will be uploaded to the IBM Stand & Deliver platform for evaluation. This video is intended to simulate your delivery of a \u201clive\u201d demo in front of a client \u2014 on camera. IBMers will have flexibility in defining a hypothetical client, the pain points that customer has, and the goals they aspire to. The recording will then cover the seller\u2019s hands-on demonstration and pitch to the client of the value of the IBM solution using the environments and techniques of this lab. BUSINESS PARTNERS Business Partners must pass an accreditation quiz after completing the hands-on portion of the course. The quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. The quiz questions will ask you about on-screen text or descriptions that come up as you work through the lab guide. Answers to quiz questions can only be determined by carefully following the instructions of the hands-on lab. Sections of the lab marked as OPTIONAL will not be tested on as part of the quiz. You will have three attempts in total to obtain a passing grade.","title":"Evaluation Criteria for IBM Technical Sellers and Business Partners"},{"location":"evaluation/#_1","text":"","title":""},{"location":"evaluation/#ibmer-stand-deliver-assessment","text":"IBMers \u2014 SUBMIT RECORDINGS HERE Submit your Stand & Deliver recording online using IBM YourLearning: https://yourlearning.ibm.com/activity/ISND-SALESENABLEMENT-111810?planId=PLAN-66BD32D205CA&sectionId=SECTION-A&planIdFromParentTab=PLAN-66BD32D205CA&sectionIdFromParentTab=SECTION-A&planIdForChildTab=PLAN-66BD32D205CA The evaluation criteria described below and on Seismic only applies to IBMers , who must record a Stand & Deliver to receive accreditation for this Level 3 course. By default, IBMers will be evaluated by their First Line Manager (FLM) \u2014 although they may request another manager to evaluate their Stand & Deliver, if appropriate. Instructions on how to submit a Stand & Deliver are included on this page. Sellers must include each of the following in their recording: Seller articulated client's pain point(s) and the value proposition of using OpenShift on IBM Power Systems. Seller highlighted use cases for OpenShift on Power. Seller demonstrated and discussed several of the key differentiated capabilities of OpenShift on Power that deliver on the value proposition on point one. Seller highlighted benefits to the client (this is the why the client can\u2019t live without these benefits section). Seller highlighted benefits to the client's customers (what will the client be able to deliver to their customers that they could not without this product). Seller closed the demo with a call to action for the client that could include: a workshop, a deeper dive into the product meeting, MVP engagements, and so on.","title":"IBMer Stand &amp; Deliver Assessment"},{"location":"evaluation/#_2","text":"","title":""},{"location":"evaluation/#business-partner-quiz-assessment","text":"PARTNERS \u2014 COMPLETE ASSESSMENT HERE Complete your assessment online using IBM Training: https://learn.ibm.com/mod/subcourse/view.php?id=217396 The accreditation quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. The quiz questions will ask you about on-screen text or descriptions that come up as you work through the lab guide. Answers to the quiz can only be determined by carefully following the instructions of the hands-on lab. Sections of the lab marked as OPTIONAL will not be tested on as part of the quiz. You will have three attempts in total to obtain a passing grade.","title":"Business Partner Quiz Assessment"},{"location":"evaluation/#_3","text":"","title":""},{"location":"evaluation/#next-steps","text":"In the following section, you will prepare a hands-on lab environment with the necessary services and configurations.","title":"Next steps"},{"location":"moduleA/","text":"Module A: Enable continuous deployment via Red Hat OpenShift S2I and GitHub webhooks In this module, we'll journey with Amy's team of developers and application builders as they explore two different ways to leverage OpenShift's S2I framework and \"webhooks\" for managing and deploying source code into live applications. There are compelling reasons to use either of the two build strategies (Docker build or source build) \u2014 and both have their unique advantages of DevOps teams, which we'll explore in detail. The Red Hat OpenShift Source-to-Image (S2I) framework eases Amy\u2019s ability to create reproducible Docker container images from application source code. A Dockerfile is a recipe (or blueprint) for building Docker images. You will learn how to use OpenShift S2I to build a Docker image from a Dockerfile hosted in GitHub and deploy a container pod by using the image. You'll also learn how to set up a GitHub webhook to notify OpenShift of new code push and commit events so it automatically rebuilds and redeploys pods by using the latest changes to your code and the Dockerfile within your GitHub repo. This module will make heavy use of GitHub , a ubiquitous tool in nearly every developer and programmer\u2019s arsenal. Individuals, enterprise organizations, and open source community projects all use GitHub to share, deploy, and collaborate on code. Before getting started, make sure you have signed up for a free GitHub account . Once registered, you will be able to begin replicating (\u201cforking\u201d) project code from our lab repository into your own account \u2014 which you will then use to demonstrate OpenShift\u2019s Source-2-Image capabilities for deploying applications via Dockerfiles (that are hosted on GitHub). Open a web browser, log in to GitHub, and open the following IBM-operated repository: https://github.com/dpkshetty/oc-docker-s2i Click the Fork icon (as shown). This will begin the process of replicating the code into your own repository. After executing a code fork, you should notice that the oc-docker-s2i repository is now available under your own GitHub account: in this example's case, bienko/oc-docker-s2i . You can now safely make changes and additions to our localized replica of the repository, without impacting the code available on the IBM repository. Keep the GitHub tab open on your Web browser \u2014 you'll be returning to it shortly \u2014 and return to the OpenShift Container Platform web interface you logged into earlier. From the main landing page of the OCP cluster web interface, locate the Administrator / Developer toggle button near the top-left of the page. You will use this toggle frequently over the course of the hands-on lab to cycle between Administrator and Developer perspectives (and privileges) within the OCP cluster. For now, make sure it is toggled to Administrator . From the tabs on the left of the page, locate Home and then drill down into Projects . Create a new Project for demonstrating OCP\u2019s Source-2-Image capabilities, so from the top-right of the page click the Create Project button. Assign labels to the Project and then click Create . Name : set to s2i-project Display name : set to s2i-project Description : set to a description of your choosing Once satisfied, click Create to confirm. After creation of the s2i-project, toggle the perspective button in the top-left to Developer . Click the Topology tab from the left navigation bar. As expected, no resources have yet to be assigned. Verify that the Project is set to the same one you created as an administrator in Step 5 ( s2i-project ). Next you want to deploy our first application to the OCP cluster. From the left navigation bar, click +Add . OpenShift supports adding code, assets, and applications from a massive gamut of sources. From the +Add page, scroll down and locate the Git Repository section. Within this grouping is the field Import from Git \u2014 select this option to open a configuration page. For the Git Repo URL , switch back to the GitHub tab to your personal repository (the one you forked previously) and copy the URL into the OCP configuration page. The URL should look something like: https://github.com/[your_account]/oc-docker-s2i Click the Show advanced Git options button just below the URL field. Enter main under Git reference . In the Context dir field , enter /redhat to specify the directory within the forked repository where the Dockerfile code is stored. Scroll down until you reach the General section. Here you can provide an Application Name and a unique Name that will be associated with the resource. Note that if an image already exists in OCP with the same name you will receive an error message \u2014 be sure to use a unique name. The next field to examine is labelled Resource Type section. Clicking the name will produce a drop-down menu with two options for the type of resource OCP will generate. Set the Resource Type to the DeploymentConfig option. RECORD THIS VALUE Record the name of the alternative resource type (hint: the one not labeled DeploymentConfig ). Target Port can be left on the default value of 8080 . Make sure that the Create a route (to the application) box is enabled before continuing. When satisfied, click the Create button. Your browser will redirect to the Topology tab, which will no longer be empty as before, but will now display a tile for the oc-docker-s2i you just configured. It will take a minute or two for the container build and deployment to complete. Wait until you see a circular green checkmark icon, which indicates Build Ready , to appear next to the application tile before continuing Click the rectangular icon containing text similar to (DC) oc-docker-s2i (your exact naming may differ), located just below the OpenShift logo at the center of the tile. Doing so will pull open an overview panel along the right side of the web interface. From this panel you can peruse a number of details on the status and health of your application. If you\u2019re quick enough, you may notice that the Builds category still shows that the application is still in the middle of being automatically deployed and orchestrated by OpenShift. In a few moments, that build order will resolve to \u201ccomplete.\u201d Likewise, containers are automatically provisioned to house and execute the code you deployed via GitHub. From within the overview panel along the right side of the interface, click the name of the application ( oc-docker-s2i , as shown) to drill down into further details and to expose information on the pod(s) that it is running across. By clicking on the name of the DeploymentConfig ( oc-docker-s2i ), you are exposed to much more granular details on the status of the application. From this panel, you can modify nearly every aspect of how the containerized application is running on the OpenShift cluster, including how many replicas are available (we\u2019ll experiment with this later), how resources are allocated, and so on. RECORD THIS VALUE On the right side of the DeploymentConfig details panel, in white text, there is a field Update Strategy which details how new app Builds are generated. Record the default value it is set to. In order to execute instructions on the oc-docker-s2i , we\u2019ll need to interface directly with the pod (container) it is running upon. From the tabs along the top of the panel, click Pods . When that page has loaded, click the Name of the pod ( oc-docker-s2i-1-852rw , for example). Within the Pod Details panel, you have the option to further inspect the configuration and status of individual containers (pods) associated with the s2i-project deployment. To interface directly with the pod and issue commands, you can access the Web interface built-in console via the Terminal tab. The console can be used to issue commands to the pod in an identical way you would use any Linux / SSH interface. Issue the following command to inspect the running output of the pod processes, which is executing the code that you originally forked from GitHub: ps aux | grep sleep SLEEP INFINITY Verify that the pod is running a sleep infinity process. This will become important at a later stage of this module. GITHUB WEBHOOKS GitHub Webhooks allow external services, such as OpenShift Container Platform, to be notified when certain events happen \u2014 like updates to application code. Amy\u2019s team is continuously pushing updates to their application code base, and being able to automatically trigger updates to live applications when her developer teams commit new source code will greatly accelerate their responsiveness and agility. To automatically deploy a new pod when updates to the GitHub Dockerfile occur, you must first configure your GitHub repository with a webhook that OpenShift provides as part of the BuildConfig . This is best achieved by using the OpenShift web interface \u2014 so let\u2019s continue using that. With the Developer perspective still active, select the Builds tab (left side) and then click the name of the running application ( oc-docker-s2i ) you wish to modify. Within the BuildConfig Details panel, scroll down to the bottom of the page where you will find the Webhooks category. There are two classes of Webhooks that are available to you: Generic and GitHub types. You will need to store the GitHub webhook details for use within GitHub shortly. Use the Copy URL with Secret button in the right-most column of the GitHub row and then store this on your notepad . Return back to the open GitHub tab from earlier. From your personal account fork of the oc-docker-s2i repository, click the Settings tab as shown. From the Settings page, click the Webhooks tab (left). If you haven\u2019t set up a webhook previously, this should display an empty page. Click the Add Webhook button. Into the Payload URL field, paste the URL captured in Step 24 into this box. Use the Content Type drop-down and correct the value to use application/json format. To prevent complications with SSL certificates, you must check the DISABLE field under SSL Verification category. However, this is not recommended in real-world scenarios. When satisfied, click the Add Webhook button. Your Web browser will be returned to the Webhooks panel, where the newly-generated webhook (denoted by what looks like an HTTPs address) will be listed. Clck the Edit button located to the right of your newly-created Webhook's name. A new panel will load with two tabs at the top: Settings and Recent Deliveries . Click the Recent Deliveries tab to validate that the webhook is working properly. There should be a green checkmark to the left of an alphanumeric string (representing a recent request or response between the webhook and OpenShift Container Platform on PowerVS). RECORD THIS VALUE Click the name of the alphanumeric string (something like 0daa9e50-90... ) to expand additional details on the delivery. Record the value of Content-Type: returned under the webhook\u2019s Request Headers . Your next step is to verify that the webhook is working as intended: monitoring changes committed to the source code in GitHub and propagating those versioning updates into the live application running via OpenShift. This will simulate an automated and agile workflow between Amy\u2019s team of developers and the live applications supporting other areas of her business. To test the webhook, first return to the main landing page of your personal account replica of the oc-docker-s2i repository (for example, https://github.com/bienko/oc-docker-s2i ). It is critical that in the steps to follow you are editing your own personal fork and not the main IBM branch that you originally replicated the code from. Click the redhat directory and then the Dockerfile asset from the subsequent page to view the code base of our running application. With the Dockerfile selected, click the pencil icon (as shown) to edit the content of the file. Comment OUT the following line (originally line 5 in the Dockerfile) by inserting a # at the start of the line: # CMD [\u201c/bin/bash\u201d, \u201c-c\u201d, \u201csleep infinity\u201d] Remove the # from the start of the following line (originally line 6 in Dockerfile) to make the instruction executable: CMD [ \u201c/bin/bash\u201d, \u201c-c\u201d, \u201c--\u201d, \u201cwhile true ; do sleep 30 ; done ; \u201d ] The resulting file should look like the following screenshot: You may add additional comments, like shown on line 5, explaining the code base changes. Scroll down until you reach the Commit Changes panel. Here you can title your code commit and add a description of the changes made. Ensure that the Commit directly to the main branch button is selected (this will update only your personal replica \u2014 not the IBM branch). When satisfied, click Commit Changes . Navigate back to your OpenShift Container Platform dashboard. Toggle the perspective changer to the Administrator view. From the Builds tab, drill down into Builds . Under the Builds panel, you will likely see multiple versions of the oc-docker-s2i codebase. Likely the changes you committed to the Dockerfile on GitHub will still be in the process of propagating to OCP \u2014 which is reflected by the new build underway ( Running ) in the screenshot shown here. When this status changes to Complete , a new build of the application is ready and has been pushed to the live version of the application. All done automatically via OpenShift\u2019s Source-2-Image framework! When the Status of the latest build of your application reads as Complete , toggle the perspective changer back to the Developer view. From the Developer perspective, click the Topology tab from the left-side menu bar. Once again you will be presented with a view of the oc-docker-s2i deployment. Click the center of the application to inspect it in detail. Under the Builds category, you can see a replica of the completed builds viewed earlier under the Administrator perspective. Your next objective will be to verify that the code base of the applications has been updated to mirror the code changes you committed to GitHub earlier. Click the name of the running Pod as shown. As before, click on the Terminal tab and enter: ps aux | grep sleep RECORD THIS VALUE Record the output from console. The output from the Terminal should reflect the code changes you committed to the Dockerfile previously. If so, congratulations \u2014 you\u2019ve successfully updated Amy\u2019s application and automatically pushed those version changes into a new live build of the service! You have successfully demonstrated OpenShift\u2019s S2I capabilities by using a Dockerfile, deployed a pod from a Dockerfile hosted in GitHub, set up connectivity between OCP and GitHub by using a webhook, and tested to ensure that the new code changes within the repository resulted in a new pod deployment within your OpenShift cluster. Shift Left, CI/CD, and Kubernetes To \u201c Shift Left ,\u201d as developers put it, is to loop back on old processes, identify short\u2010 comings within the old ways of doing things, and steadily iterate on those designs to improve them through experience over time. Plenty of lessons are learned each year within the IT marketplace, and the pandemic years are certainly no exception. The disruption underway from this monumental shift in computing, as we enter the new decade, cannot be understated. How can a business have confidence that its choice of technology partner in this new paradigm is the correct one? The Japanese word \u30db\u309a\u30ab\u30e8\u30b1 ( Poka yoke ) literally means \u201cmistake-proofing\u201d \u2014 its goal is to prevent inadvertent errors and eliminate product defects through early prevention and correction. For developers, the term shift left is a practice intended to find and prevent defects in the software development lifecycle. Left reflects the iterative loop that defines such software development cycles, putting forth changes that are tested and evaluated, again and again. The concept of Shift Left application development also wonderfully mirrors the same rigorous methodology of revisiting and questioning a null hypothesis using the scientific method. Shifting back to the left\u2014pulling through again with a finer-toothed comb\u2014and continuing to refine on the design (rather than pushing straight through to release with the first working prototype) puts the values of Poka yoke into practice. The ability to run an application in the cloud, in a way that\u2019s independent of any cloud vendor\u2014where your own datacenter or machine room can be one of those \u201cvendors\u201d\u2014is a huge step forward. But there\u2019s more to the problem than running the application. You need the ability to deploy it, you need the ability to integrate components together, and you need the ability to test in a modern way. What\u2019s more, these capabilities all need to be automated\u2014in part so they can be repeated reliably, but also so they can be performed repeatedly. All of these capabilities come under the heading of Continuous Integration & Continuous Delivery (CI/CD). The deployment scenarios of 20 or 30 years ago, when a \u201cdeployment\u201d was very likely to be a break-the-world change, with the entire development and operations teams keeping their fingers crossed and hoping nothing breaks, are a thing of the past. We\u2019ve discovered that the way to deploy software reliably is to deploy it frequently, where each deployment represents a minor change to a very small number of features. If each release represents a minor change, changes are easy to roll back; changes can be deployed to a small number of users for testing; and, most of all, short, reliable release cycles force you to commit your release process to software. As mentioned before, Kubernetes (K8s) provides the building blocks of a PaaS\u2014but it\u2019s not an all-inclusive PaaS. One of the things Kubernetes explicitly doesn\u2019t do is provide a native CI/CD process. In open source parlance, Kubernetes doesn\u2019t have an opinion on CI/CD\u2014which is coder talk for they don\u2019t force you into a solution or template for this component; this can be good or bad. After all, developers definitely have formed their own opinions in this space, and DevOps processes tend to be custom fit to the team that implements them. This is one of the reasons that we think Kubernetes has become such an inflection point \u2014 K8s knows what it wants to do and it\u2019s extremely good at doing it. For everything else, the open source community always finds a way. One way that Kubernetes-based platforms such as Red Hat OpenShift make it easy to collaborate with the open source community\u2014 and readily deploy open source code into production environments \u2014is via the Source-2-Image framework. In the following steps, you'll explore how Anne and her team can leverage such a framework to the benefit of her business. Source Build Amy\u2019s team has now successfully deployed application code via what we\u2019ve coined the \u201dDocker build\u201d strategy. But what if her teams of DevOps engineers and developers want to leverage webhooks for working (and deploying) directly from the source code? Rather than needing to manually import source code from repositories such as GitHub\u2014 like you did in the previous steps \u2014what if Amy could produce the same results with minimal micromanagement and better automation? In the following steps, you'll reproduce a similar workflow to what you just completed (using a sample pyFlask web framework application hosted on GitHub), but this time using webhooks to automatically notify OpenShift of new code push/commit events in GitHub . The result will be that OpenShift automatically rebuilds and redeploys the application using the latest code changes in your GitHub repository, without much (if any) intervention from the DevOps team (you). Let\u2019s get started. In your web browser, open the OpenShift cluster dashboard and switch to the Administrator perspective. From the main landing page, drill down into Projects . Click the Create Project button and assign it the name pyflask-demo , then click Create when satisfied. Verify that the project has been created by returning to the Projects screen and searching for the project by name. Your next step will be to replicate a simple pyFlask application\u2014 consisting of three API endpoints (root, version, and test) \u2014which is hosted at the following repo: https://github.com/ocp-power-demos/s2i-pyflask-demo Open a new tab in your web browser and navigate to your personal GitHub account (created earlier). Then visit the link above and click Fork to replicate the code into your own GitHub repository. This will ensure that any changes made to the GitHub code only impact your own replica, rather than the main parent branch. Return to the OpenShift cluster dashboard and switch to the Developer perspective. From the taskbar on the left, select the +Add button and then click the From Git option (located under the Git Repository category). Note that when you completed the earlier component of this lab module, you selected the From Dockerfile option. Paste the URL of your personal GitHub account\u2019s fork of the pyFlask application into the Git Repo URL field. OpenShift will automatically validate the URL and the Builder Image should display as a Python image. Click the Show Advanced Git Options toggle. For the Git Reference field, enter the value main . Scroll down to General and set s2i-pyflask for both the Application Name and Name fields. Under the Resources section, make sure that the Deployment option is selected. Scrolling further down under Advanced Options , ensure that you enable the Create a Route to the Application button. Follow this up with enabling the Show advanced Routing options toggle. Leave the first several fields as their default settings. Scroll down until you reach Security and then enable the Secure Route box. Further down, set the TLS Termination value to Edge and set Insecure Traffic to None . These modifications are necessary as OpenShift on Power Virtual Server by default only supports secure (HTTPs) routes by default. When satisfied, click Create . Your web browser will automatically redirect to the Topology page. From here, you will see a new application tile for your freshly-configured pyFlask service. Click the center of the application icon to pull open an overview. It will take several moments for the Build to configure and deploy \u2014 wait until the Build status reads as complete before moving to the next step. After the Build is completed, find the Routes section at the bottom of the panel and click the HTTPs URL to be directed to a live view of the running application. A new tab should open within your web browser displaying a message identical to the one presented in the following screenshot. Test the other two API endpoints that were part of the original pyFlask application code base that you replicated from GitHub, in order to validate that the full application was brought over and has correctly deployed. Add /version to the end of the URL for your application\u2019s web address (Step 54) and then hit Return on your keyboard to load the new page. Validate that the /test endpoint is also working. RECORD THIS VALUE Record the message displayed to screen when validating the /test endpoint. If all three endpoints are working then OpenShift has successfully deployed a containerized application pod directly from our GitHub repository. Return to the OpenShift cluster dashboard and switch to the Developer perspective. Drill down into the Builds tab, in which you should see an s2i-pyflask service. Click the name of the service. From the service details panel that opens, scroll down to the bottom of the page and locate the Webhooks section. There are two types\u2014 Generic and GitHub \u2014which are displayed. At the far right of the GitHub row, click the Copy URL with Secret button. In a separate web browser tab, navigate to your personal GitHub account page. From the tabs along the top of the page, click the Settings tab (far right) and then Webhooks from the list of panels that appear on the left side of the new page. You might see existing Webhooks on this page already. Ignore any pre-existing Webhooks and generate a new one using the Add Webhook button. Under the Payload URL field, paste the address that was copied in Step 57. Update the Content Type field to application/json Click the Disable option under the SSL Verification category (you will be asked to confirm this selection). When satisfied, click Add Webhook . The web browser will redirect back to the Webhooks page. It will take a few moments for the webhook to initialize and make its first deliveries to the OpenShift cluster. You will have confirmation that the webhook is working properly once a green checkmark and a Last delivery was successful statement appear on screen. To inspect additional details about the webhook (and its deliveries), click the webhook name and scroll down to the bottom of the page, then click the icon as shown. RECORD THIS VALUE Record value of Content-Type: that is displayed within the webhook Headers field Let\u2019s now make a small change to the source code by changing the version to 2.0 in app.py . This should trigger a push event from GitHub to OpenShift, and in turn cause OpenShift to rebuild the Docker image and redeploy our pod using the newly-built Docker image. Return to the main landing page of your personal account replica of s2i-pyflask-demo and drill down into the app.py file within the directory of assets. Edit the code of the Python script using the pencil icon . On line 10 of the application code, change the version number from 1.0 to 2.0 ; afterwards, scroll down and click Commit Changes with the Commit directly to the main branch option selected. You may add comments describing the commit changes if you wish. Go back to the OpenShift cluster dashboard and open the Topology page. Click the center of the s2i-pyflask application icon to pull up additional details on the app. Notice that a new Build has automatically been initiated (and may have already finished by the time you check this screen) as a result of the webhook\u2019s two-pronged workflow: recognition of the changes you made to the application source code in GitHub; and immediately launching a new pod (running the updated code) for the s2i-pyflask application. As before, scroll down the panel until you reach the Routes section. Click the URL to open a new tab with the updated s2i-pyflask application. Validate the changes made to the /version endpoint. Note that the Route URL is unchanged from having updated the application between version 1 to version 2. If you already have the /version endpoint URL opened, you can refresh it to update the application view. Congratulations, you have successfully used OpenShift's S2I capabilities on an OCP cluster running in Power Virtual Server. Amy's team have been able deploy an application to their OpenShift cluster directly from a GitHub-hosted source code, set up connectivity between OpenShift and GitHub using webhooks, and ensure that the new code changes to the source code repository resulted in a new version of the application being deployed automatically to the OpenShift cluster. Next Steps In the following module, you will learn (alongside Amy's team) how to manage a basic application's lifecycle via OpenShift using a graphical interface method.","title":"Module A - Red Hat OpenShift S2I and GitHub webhooks"},{"location":"moduleA/#module-a-enable-continuous-deployment-via-red-hat-openshift-s2i-and-github-webhooks","text":"In this module, we'll journey with Amy's team of developers and application builders as they explore two different ways to leverage OpenShift's S2I framework and \"webhooks\" for managing and deploying source code into live applications. There are compelling reasons to use either of the two build strategies (Docker build or source build) \u2014 and both have their unique advantages of DevOps teams, which we'll explore in detail. The Red Hat OpenShift Source-to-Image (S2I) framework eases Amy\u2019s ability to create reproducible Docker container images from application source code. A Dockerfile is a recipe (or blueprint) for building Docker images. You will learn how to use OpenShift S2I to build a Docker image from a Dockerfile hosted in GitHub and deploy a container pod by using the image. You'll also learn how to set up a GitHub webhook to notify OpenShift of new code push and commit events so it automatically rebuilds and redeploys pods by using the latest changes to your code and the Dockerfile within your GitHub repo. This module will make heavy use of GitHub , a ubiquitous tool in nearly every developer and programmer\u2019s arsenal. Individuals, enterprise organizations, and open source community projects all use GitHub to share, deploy, and collaborate on code. Before getting started, make sure you have signed up for a free GitHub account . Once registered, you will be able to begin replicating (\u201cforking\u201d) project code from our lab repository into your own account \u2014 which you will then use to demonstrate OpenShift\u2019s Source-2-Image capabilities for deploying applications via Dockerfiles (that are hosted on GitHub). Open a web browser, log in to GitHub, and open the following IBM-operated repository: https://github.com/dpkshetty/oc-docker-s2i Click the Fork icon (as shown). This will begin the process of replicating the code into your own repository. After executing a code fork, you should notice that the oc-docker-s2i repository is now available under your own GitHub account: in this example's case, bienko/oc-docker-s2i . You can now safely make changes and additions to our localized replica of the repository, without impacting the code available on the IBM repository. Keep the GitHub tab open on your Web browser \u2014 you'll be returning to it shortly \u2014 and return to the OpenShift Container Platform web interface you logged into earlier. From the main landing page of the OCP cluster web interface, locate the Administrator / Developer toggle button near the top-left of the page. You will use this toggle frequently over the course of the hands-on lab to cycle between Administrator and Developer perspectives (and privileges) within the OCP cluster. For now, make sure it is toggled to Administrator . From the tabs on the left of the page, locate Home and then drill down into Projects . Create a new Project for demonstrating OCP\u2019s Source-2-Image capabilities, so from the top-right of the page click the Create Project button. Assign labels to the Project and then click Create . Name : set to s2i-project Display name : set to s2i-project Description : set to a description of your choosing Once satisfied, click Create to confirm. After creation of the s2i-project, toggle the perspective button in the top-left to Developer . Click the Topology tab from the left navigation bar. As expected, no resources have yet to be assigned. Verify that the Project is set to the same one you created as an administrator in Step 5 ( s2i-project ). Next you want to deploy our first application to the OCP cluster. From the left navigation bar, click +Add . OpenShift supports adding code, assets, and applications from a massive gamut of sources. From the +Add page, scroll down and locate the Git Repository section. Within this grouping is the field Import from Git \u2014 select this option to open a configuration page. For the Git Repo URL , switch back to the GitHub tab to your personal repository (the one you forked previously) and copy the URL into the OCP configuration page. The URL should look something like: https://github.com/[your_account]/oc-docker-s2i Click the Show advanced Git options button just below the URL field. Enter main under Git reference . In the Context dir field , enter /redhat to specify the directory within the forked repository where the Dockerfile code is stored. Scroll down until you reach the General section. Here you can provide an Application Name and a unique Name that will be associated with the resource. Note that if an image already exists in OCP with the same name you will receive an error message \u2014 be sure to use a unique name. The next field to examine is labelled Resource Type section. Clicking the name will produce a drop-down menu with two options for the type of resource OCP will generate. Set the Resource Type to the DeploymentConfig option. RECORD THIS VALUE Record the name of the alternative resource type (hint: the one not labeled DeploymentConfig ). Target Port can be left on the default value of 8080 . Make sure that the Create a route (to the application) box is enabled before continuing. When satisfied, click the Create button. Your browser will redirect to the Topology tab, which will no longer be empty as before, but will now display a tile for the oc-docker-s2i you just configured. It will take a minute or two for the container build and deployment to complete. Wait until you see a circular green checkmark icon, which indicates Build Ready , to appear next to the application tile before continuing Click the rectangular icon containing text similar to (DC) oc-docker-s2i (your exact naming may differ), located just below the OpenShift logo at the center of the tile. Doing so will pull open an overview panel along the right side of the web interface. From this panel you can peruse a number of details on the status and health of your application. If you\u2019re quick enough, you may notice that the Builds category still shows that the application is still in the middle of being automatically deployed and orchestrated by OpenShift. In a few moments, that build order will resolve to \u201ccomplete.\u201d Likewise, containers are automatically provisioned to house and execute the code you deployed via GitHub. From within the overview panel along the right side of the interface, click the name of the application ( oc-docker-s2i , as shown) to drill down into further details and to expose information on the pod(s) that it is running across. By clicking on the name of the DeploymentConfig ( oc-docker-s2i ), you are exposed to much more granular details on the status of the application. From this panel, you can modify nearly every aspect of how the containerized application is running on the OpenShift cluster, including how many replicas are available (we\u2019ll experiment with this later), how resources are allocated, and so on. RECORD THIS VALUE On the right side of the DeploymentConfig details panel, in white text, there is a field Update Strategy which details how new app Builds are generated. Record the default value it is set to. In order to execute instructions on the oc-docker-s2i , we\u2019ll need to interface directly with the pod (container) it is running upon. From the tabs along the top of the panel, click Pods . When that page has loaded, click the Name of the pod ( oc-docker-s2i-1-852rw , for example). Within the Pod Details panel, you have the option to further inspect the configuration and status of individual containers (pods) associated with the s2i-project deployment. To interface directly with the pod and issue commands, you can access the Web interface built-in console via the Terminal tab. The console can be used to issue commands to the pod in an identical way you would use any Linux / SSH interface. Issue the following command to inspect the running output of the pod processes, which is executing the code that you originally forked from GitHub: ps aux | grep sleep SLEEP INFINITY Verify that the pod is running a sleep infinity process. This will become important at a later stage of this module. GITHUB WEBHOOKS GitHub Webhooks allow external services, such as OpenShift Container Platform, to be notified when certain events happen \u2014 like updates to application code. Amy\u2019s team is continuously pushing updates to their application code base, and being able to automatically trigger updates to live applications when her developer teams commit new source code will greatly accelerate their responsiveness and agility. To automatically deploy a new pod when updates to the GitHub Dockerfile occur, you must first configure your GitHub repository with a webhook that OpenShift provides as part of the BuildConfig . This is best achieved by using the OpenShift web interface \u2014 so let\u2019s continue using that. With the Developer perspective still active, select the Builds tab (left side) and then click the name of the running application ( oc-docker-s2i ) you wish to modify. Within the BuildConfig Details panel, scroll down to the bottom of the page where you will find the Webhooks category. There are two classes of Webhooks that are available to you: Generic and GitHub types. You will need to store the GitHub webhook details for use within GitHub shortly. Use the Copy URL with Secret button in the right-most column of the GitHub row and then store this on your notepad . Return back to the open GitHub tab from earlier. From your personal account fork of the oc-docker-s2i repository, click the Settings tab as shown. From the Settings page, click the Webhooks tab (left). If you haven\u2019t set up a webhook previously, this should display an empty page. Click the Add Webhook button. Into the Payload URL field, paste the URL captured in Step 24 into this box. Use the Content Type drop-down and correct the value to use application/json format. To prevent complications with SSL certificates, you must check the DISABLE field under SSL Verification category. However, this is not recommended in real-world scenarios. When satisfied, click the Add Webhook button. Your Web browser will be returned to the Webhooks panel, where the newly-generated webhook (denoted by what looks like an HTTPs address) will be listed. Clck the Edit button located to the right of your newly-created Webhook's name. A new panel will load with two tabs at the top: Settings and Recent Deliveries . Click the Recent Deliveries tab to validate that the webhook is working properly. There should be a green checkmark to the left of an alphanumeric string (representing a recent request or response between the webhook and OpenShift Container Platform on PowerVS). RECORD THIS VALUE Click the name of the alphanumeric string (something like 0daa9e50-90... ) to expand additional details on the delivery. Record the value of Content-Type: returned under the webhook\u2019s Request Headers . Your next step is to verify that the webhook is working as intended: monitoring changes committed to the source code in GitHub and propagating those versioning updates into the live application running via OpenShift. This will simulate an automated and agile workflow between Amy\u2019s team of developers and the live applications supporting other areas of her business. To test the webhook, first return to the main landing page of your personal account replica of the oc-docker-s2i repository (for example, https://github.com/bienko/oc-docker-s2i ). It is critical that in the steps to follow you are editing your own personal fork and not the main IBM branch that you originally replicated the code from. Click the redhat directory and then the Dockerfile asset from the subsequent page to view the code base of our running application. With the Dockerfile selected, click the pencil icon (as shown) to edit the content of the file. Comment OUT the following line (originally line 5 in the Dockerfile) by inserting a # at the start of the line: # CMD [\u201c/bin/bash\u201d, \u201c-c\u201d, \u201csleep infinity\u201d] Remove the # from the start of the following line (originally line 6 in Dockerfile) to make the instruction executable: CMD [ \u201c/bin/bash\u201d, \u201c-c\u201d, \u201c--\u201d, \u201cwhile true ; do sleep 30 ; done ; \u201d ] The resulting file should look like the following screenshot: You may add additional comments, like shown on line 5, explaining the code base changes. Scroll down until you reach the Commit Changes panel. Here you can title your code commit and add a description of the changes made. Ensure that the Commit directly to the main branch button is selected (this will update only your personal replica \u2014 not the IBM branch). When satisfied, click Commit Changes . Navigate back to your OpenShift Container Platform dashboard. Toggle the perspective changer to the Administrator view. From the Builds tab, drill down into Builds . Under the Builds panel, you will likely see multiple versions of the oc-docker-s2i codebase. Likely the changes you committed to the Dockerfile on GitHub will still be in the process of propagating to OCP \u2014 which is reflected by the new build underway ( Running ) in the screenshot shown here. When this status changes to Complete , a new build of the application is ready and has been pushed to the live version of the application. All done automatically via OpenShift\u2019s Source-2-Image framework! When the Status of the latest build of your application reads as Complete , toggle the perspective changer back to the Developer view. From the Developer perspective, click the Topology tab from the left-side menu bar. Once again you will be presented with a view of the oc-docker-s2i deployment. Click the center of the application to inspect it in detail. Under the Builds category, you can see a replica of the completed builds viewed earlier under the Administrator perspective. Your next objective will be to verify that the code base of the applications has been updated to mirror the code changes you committed to GitHub earlier. Click the name of the running Pod as shown. As before, click on the Terminal tab and enter: ps aux | grep sleep RECORD THIS VALUE Record the output from console. The output from the Terminal should reflect the code changes you committed to the Dockerfile previously. If so, congratulations \u2014 you\u2019ve successfully updated Amy\u2019s application and automatically pushed those version changes into a new live build of the service! You have successfully demonstrated OpenShift\u2019s S2I capabilities by using a Dockerfile, deployed a pod from a Dockerfile hosted in GitHub, set up connectivity between OCP and GitHub by using a webhook, and tested to ensure that the new code changes within the repository resulted in a new pod deployment within your OpenShift cluster.","title":"Module A: Enable continuous deployment via Red Hat OpenShift S2I and GitHub webhooks"},{"location":"moduleA/#_1","text":"","title":""},{"location":"moduleA/#shift-left-cicd-and-kubernetes","text":"To \u201c Shift Left ,\u201d as developers put it, is to loop back on old processes, identify short\u2010 comings within the old ways of doing things, and steadily iterate on those designs to improve them through experience over time. Plenty of lessons are learned each year within the IT marketplace, and the pandemic years are certainly no exception. The disruption underway from this monumental shift in computing, as we enter the new decade, cannot be understated. How can a business have confidence that its choice of technology partner in this new paradigm is the correct one? The Japanese word \u30db\u309a\u30ab\u30e8\u30b1 ( Poka yoke ) literally means \u201cmistake-proofing\u201d \u2014 its goal is to prevent inadvertent errors and eliminate product defects through early prevention and correction. For developers, the term shift left is a practice intended to find and prevent defects in the software development lifecycle. Left reflects the iterative loop that defines such software development cycles, putting forth changes that are tested and evaluated, again and again. The concept of Shift Left application development also wonderfully mirrors the same rigorous methodology of revisiting and questioning a null hypothesis using the scientific method. Shifting back to the left\u2014pulling through again with a finer-toothed comb\u2014and continuing to refine on the design (rather than pushing straight through to release with the first working prototype) puts the values of Poka yoke into practice. The ability to run an application in the cloud, in a way that\u2019s independent of any cloud vendor\u2014where your own datacenter or machine room can be one of those \u201cvendors\u201d\u2014is a huge step forward. But there\u2019s more to the problem than running the application. You need the ability to deploy it, you need the ability to integrate components together, and you need the ability to test in a modern way. What\u2019s more, these capabilities all need to be automated\u2014in part so they can be repeated reliably, but also so they can be performed repeatedly. All of these capabilities come under the heading of Continuous Integration & Continuous Delivery (CI/CD). The deployment scenarios of 20 or 30 years ago, when a \u201cdeployment\u201d was very likely to be a break-the-world change, with the entire development and operations teams keeping their fingers crossed and hoping nothing breaks, are a thing of the past. We\u2019ve discovered that the way to deploy software reliably is to deploy it frequently, where each deployment represents a minor change to a very small number of features. If each release represents a minor change, changes are easy to roll back; changes can be deployed to a small number of users for testing; and, most of all, short, reliable release cycles force you to commit your release process to software. As mentioned before, Kubernetes (K8s) provides the building blocks of a PaaS\u2014but it\u2019s not an all-inclusive PaaS. One of the things Kubernetes explicitly doesn\u2019t do is provide a native CI/CD process. In open source parlance, Kubernetes doesn\u2019t have an opinion on CI/CD\u2014which is coder talk for they don\u2019t force you into a solution or template for this component; this can be good or bad. After all, developers definitely have formed their own opinions in this space, and DevOps processes tend to be custom fit to the team that implements them. This is one of the reasons that we think Kubernetes has become such an inflection point \u2014 K8s knows what it wants to do and it\u2019s extremely good at doing it. For everything else, the open source community always finds a way. One way that Kubernetes-based platforms such as Red Hat OpenShift make it easy to collaborate with the open source community\u2014 and readily deploy open source code into production environments \u2014is via the Source-2-Image framework. In the following steps, you'll explore how Anne and her team can leverage such a framework to the benefit of her business.","title":"Shift Left, CI/CD, and Kubernetes"},{"location":"moduleA/#_2","text":"","title":""},{"location":"moduleA/#source-build","text":"Amy\u2019s team has now successfully deployed application code via what we\u2019ve coined the \u201dDocker build\u201d strategy. But what if her teams of DevOps engineers and developers want to leverage webhooks for working (and deploying) directly from the source code? Rather than needing to manually import source code from repositories such as GitHub\u2014 like you did in the previous steps \u2014what if Amy could produce the same results with minimal micromanagement and better automation? In the following steps, you'll reproduce a similar workflow to what you just completed (using a sample pyFlask web framework application hosted on GitHub), but this time using webhooks to automatically notify OpenShift of new code push/commit events in GitHub . The result will be that OpenShift automatically rebuilds and redeploys the application using the latest code changes in your GitHub repository, without much (if any) intervention from the DevOps team (you). Let\u2019s get started. In your web browser, open the OpenShift cluster dashboard and switch to the Administrator perspective. From the main landing page, drill down into Projects . Click the Create Project button and assign it the name pyflask-demo , then click Create when satisfied. Verify that the project has been created by returning to the Projects screen and searching for the project by name. Your next step will be to replicate a simple pyFlask application\u2014 consisting of three API endpoints (root, version, and test) \u2014which is hosted at the following repo: https://github.com/ocp-power-demos/s2i-pyflask-demo Open a new tab in your web browser and navigate to your personal GitHub account (created earlier). Then visit the link above and click Fork to replicate the code into your own GitHub repository. This will ensure that any changes made to the GitHub code only impact your own replica, rather than the main parent branch. Return to the OpenShift cluster dashboard and switch to the Developer perspective. From the taskbar on the left, select the +Add button and then click the From Git option (located under the Git Repository category). Note that when you completed the earlier component of this lab module, you selected the From Dockerfile option. Paste the URL of your personal GitHub account\u2019s fork of the pyFlask application into the Git Repo URL field. OpenShift will automatically validate the URL and the Builder Image should display as a Python image. Click the Show Advanced Git Options toggle. For the Git Reference field, enter the value main . Scroll down to General and set s2i-pyflask for both the Application Name and Name fields. Under the Resources section, make sure that the Deployment option is selected. Scrolling further down under Advanced Options , ensure that you enable the Create a Route to the Application button. Follow this up with enabling the Show advanced Routing options toggle. Leave the first several fields as their default settings. Scroll down until you reach Security and then enable the Secure Route box. Further down, set the TLS Termination value to Edge and set Insecure Traffic to None . These modifications are necessary as OpenShift on Power Virtual Server by default only supports secure (HTTPs) routes by default. When satisfied, click Create . Your web browser will automatically redirect to the Topology page. From here, you will see a new application tile for your freshly-configured pyFlask service. Click the center of the application icon to pull open an overview. It will take several moments for the Build to configure and deploy \u2014 wait until the Build status reads as complete before moving to the next step. After the Build is completed, find the Routes section at the bottom of the panel and click the HTTPs URL to be directed to a live view of the running application. A new tab should open within your web browser displaying a message identical to the one presented in the following screenshot. Test the other two API endpoints that were part of the original pyFlask application code base that you replicated from GitHub, in order to validate that the full application was brought over and has correctly deployed. Add /version to the end of the URL for your application\u2019s web address (Step 54) and then hit Return on your keyboard to load the new page. Validate that the /test endpoint is also working. RECORD THIS VALUE Record the message displayed to screen when validating the /test endpoint. If all three endpoints are working then OpenShift has successfully deployed a containerized application pod directly from our GitHub repository. Return to the OpenShift cluster dashboard and switch to the Developer perspective. Drill down into the Builds tab, in which you should see an s2i-pyflask service. Click the name of the service. From the service details panel that opens, scroll down to the bottom of the page and locate the Webhooks section. There are two types\u2014 Generic and GitHub \u2014which are displayed. At the far right of the GitHub row, click the Copy URL with Secret button. In a separate web browser tab, navigate to your personal GitHub account page. From the tabs along the top of the page, click the Settings tab (far right) and then Webhooks from the list of panels that appear on the left side of the new page. You might see existing Webhooks on this page already. Ignore any pre-existing Webhooks and generate a new one using the Add Webhook button. Under the Payload URL field, paste the address that was copied in Step 57. Update the Content Type field to application/json Click the Disable option under the SSL Verification category (you will be asked to confirm this selection). When satisfied, click Add Webhook . The web browser will redirect back to the Webhooks page. It will take a few moments for the webhook to initialize and make its first deliveries to the OpenShift cluster. You will have confirmation that the webhook is working properly once a green checkmark and a Last delivery was successful statement appear on screen. To inspect additional details about the webhook (and its deliveries), click the webhook name and scroll down to the bottom of the page, then click the icon as shown. RECORD THIS VALUE Record value of Content-Type: that is displayed within the webhook Headers field Let\u2019s now make a small change to the source code by changing the version to 2.0 in app.py . This should trigger a push event from GitHub to OpenShift, and in turn cause OpenShift to rebuild the Docker image and redeploy our pod using the newly-built Docker image. Return to the main landing page of your personal account replica of s2i-pyflask-demo and drill down into the app.py file within the directory of assets. Edit the code of the Python script using the pencil icon . On line 10 of the application code, change the version number from 1.0 to 2.0 ; afterwards, scroll down and click Commit Changes with the Commit directly to the main branch option selected. You may add comments describing the commit changes if you wish. Go back to the OpenShift cluster dashboard and open the Topology page. Click the center of the s2i-pyflask application icon to pull up additional details on the app. Notice that a new Build has automatically been initiated (and may have already finished by the time you check this screen) as a result of the webhook\u2019s two-pronged workflow: recognition of the changes you made to the application source code in GitHub; and immediately launching a new pod (running the updated code) for the s2i-pyflask application. As before, scroll down the panel until you reach the Routes section. Click the URL to open a new tab with the updated s2i-pyflask application. Validate the changes made to the /version endpoint. Note that the Route URL is unchanged from having updated the application between version 1 to version 2. If you already have the /version endpoint URL opened, you can refresh it to update the application view. Congratulations, you have successfully used OpenShift's S2I capabilities on an OCP cluster running in Power Virtual Server. Amy's team have been able deploy an application to their OpenShift cluster directly from a GitHub-hosted source code, set up connectivity between OpenShift and GitHub using webhooks, and ensure that the new code changes to the source code repository resulted in a new version of the application being deployed automatically to the OpenShift cluster.","title":"Source Build"},{"location":"moduleA/#_3","text":"","title":""},{"location":"moduleA/#next-steps","text":"In the following module, you will learn (alongside Amy's team) how to manage a basic application's lifecycle via OpenShift using a graphical interface method.","title":"Next Steps"},{"location":"moduleB/","text":"","title":"Module B - Graphically work with Red Hat OpenShift"},{"location":"moduleC/","text":"","title":"Module C - Programmatically manipulate Red Hat OpenShift"},{"location":"setup/","text":"Prerequisites and Setup Hands-on demonstration environments will be provisioned free-of-charge from the IBM Technology Zone (ITZ). Reserve only for the time you need. These are finite resource and limiting your reservation time ensures that more IBMers and business partners can leverage the resources for client engagements. RESERVE AN ENVIRONMENT https://techzone.ibm.com/collection/modernizing-business-for-hybrid-cloud-with-ibm-power Visit the ITZ collection with your web browser. From the interface along the left-hand side, click Environments . The page will refresh and present you with a DO IT - Deploy Red Hat OpenShift on ITZ V2 tile. Click the blue Reserve button to navigate to the request form. Select the option to Reserve Now . Supply the following details into the request form: Name : Set to a unique value of your choosing Purpose : Set to Practice / Self-Education Sales Opportunity Number : For the purposes of the hands-on Level 3, you can leave this blank. However, in accordance with the IBM Technology Zone terms of service, all client demonstrations that make use of ITZ resources must be accompanied by a genuine Sales Opportunity number. Purpose Description : Please describe how the ITZ reservation will be used Preferred Geography : Set to the data center located closest to you End Date and Time : Automatically set 48 hours from the time the request is made Deployment Pattern : Automatically set to OpenShift 4.12 on POWER9 (Bastion with Single Master Node) Once satisfied, click the Check Availability button (a grey button in the bottom-left corner of the form). If the resources are available for that time slot, check the Terms and Conditions box in the bottom-right corner of the interface and then finalize the deployment by clicking Submit . Deployments can sometimes take several (2-3) hours to complete from the time that the request has been made to the ITZ. Therefore, plan your deployment and lab time accordingly. You will likely need to wait a couple of hours before you can get to work on the hands-on components of this lab. When the environment is ready, you will receive a confirmation email from the ITZ and the status of the environment under the ITZ's My Reservations tab will be marked as Status: Ready . Follow the URL provided in the email or click the tile under the My Reservations tab to drill down into further details for accessing the environment. Scroll down to the bottom of the page and take note of the following elements: OpenShift Console URL : Use this address to access the OpenShift web interface User Account : This will be set to cecuser for all environments User Password : This value will be unique to your reservation Access the OpenShift Console URL within your web browser. THIS CONNECTION IS NOT PRIVATE Depending on your web browser, you may receive a warning about This connection is not private or a similarly phrased message. Disregard these warnings and continue to the intended destination by clicking \"visit this website.\" Click the option to log in with htpasswd . For Username , supply cecuser . For Password , enter the User Password value you recorded in Step 7. Click the Log In button to proceed. Once authenticated, the web browser will redirect to the main dashboard for the OpenShift Container Platform web interface. You are now ready to proceed with the hands-on lab modules. Next Steps In the following section, you will explore two different ways to leverage OpenShift's S2I framework and \"webhooks\" for managing and deploying source code into live applications. COMMAND LINE ACCESS For parts of the hands-on lab, you will be required to establish connections over SSH to remote infrastructure endpoints. It is recommended that users do so via Terminal (Mac) or PuTTY (Windows). For detailed instructions on how to connect with PuTTY, or if you are using a Windows machine, reference the linked material .","title":"Setup"},{"location":"setup/#prerequisites-and-setup","text":"Hands-on demonstration environments will be provisioned free-of-charge from the IBM Technology Zone (ITZ). Reserve only for the time you need. These are finite resource and limiting your reservation time ensures that more IBMers and business partners can leverage the resources for client engagements. RESERVE AN ENVIRONMENT https://techzone.ibm.com/collection/modernizing-business-for-hybrid-cloud-with-ibm-power Visit the ITZ collection with your web browser. From the interface along the left-hand side, click Environments . The page will refresh and present you with a DO IT - Deploy Red Hat OpenShift on ITZ V2 tile. Click the blue Reserve button to navigate to the request form. Select the option to Reserve Now . Supply the following details into the request form: Name : Set to a unique value of your choosing Purpose : Set to Practice / Self-Education Sales Opportunity Number : For the purposes of the hands-on Level 3, you can leave this blank. However, in accordance with the IBM Technology Zone terms of service, all client demonstrations that make use of ITZ resources must be accompanied by a genuine Sales Opportunity number. Purpose Description : Please describe how the ITZ reservation will be used Preferred Geography : Set to the data center located closest to you End Date and Time : Automatically set 48 hours from the time the request is made Deployment Pattern : Automatically set to OpenShift 4.12 on POWER9 (Bastion with Single Master Node) Once satisfied, click the Check Availability button (a grey button in the bottom-left corner of the form). If the resources are available for that time slot, check the Terms and Conditions box in the bottom-right corner of the interface and then finalize the deployment by clicking Submit . Deployments can sometimes take several (2-3) hours to complete from the time that the request has been made to the ITZ. Therefore, plan your deployment and lab time accordingly. You will likely need to wait a couple of hours before you can get to work on the hands-on components of this lab. When the environment is ready, you will receive a confirmation email from the ITZ and the status of the environment under the ITZ's My Reservations tab will be marked as Status: Ready . Follow the URL provided in the email or click the tile under the My Reservations tab to drill down into further details for accessing the environment. Scroll down to the bottom of the page and take note of the following elements: OpenShift Console URL : Use this address to access the OpenShift web interface User Account : This will be set to cecuser for all environments User Password : This value will be unique to your reservation Access the OpenShift Console URL within your web browser. THIS CONNECTION IS NOT PRIVATE Depending on your web browser, you may receive a warning about This connection is not private or a similarly phrased message. Disregard these warnings and continue to the intended destination by clicking \"visit this website.\" Click the option to log in with htpasswd . For Username , supply cecuser . For Password , enter the User Password value you recorded in Step 7. Click the Log In button to proceed. Once authenticated, the web browser will redirect to the main dashboard for the OpenShift Container Platform web interface. You are now ready to proceed with the hands-on lab modules.","title":"Prerequisites and Setup"},{"location":"setup/#_1","text":"","title":""},{"location":"setup/#next-steps","text":"In the following section, you will explore two different ways to leverage OpenShift's S2I framework and \"webhooks\" for managing and deploying source code into live applications. COMMAND LINE ACCESS For parts of the hands-on lab, you will be required to establish connections over SSH to remote infrastructure endpoints. It is recommended that users do so via Terminal (Mac) or PuTTY (Windows). For detailed instructions on how to connect with PuTTY, or if you are using a Windows machine, reference the linked material .","title":"Next Steps"}]}